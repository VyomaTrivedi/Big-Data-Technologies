The first accuracy is the one using kmm and the second is one without kmm

1. ACC:87%, Total:80/80 with positive 70
   ACC:88%, Total:80/80 with positive 71

2. ACC:86%, Total:80/80 with positive 69
   ACC:86%, Total:80/80 with positive 69

3. ACC:87%, Total:80/80 with positive 70
   ACC:93%, Total:80/80 with positive 75

4. ACC:75%, Total:400/400 with positive 303
   ACC:84%, Total:400/400 with positive 336

5. ACC:75%, Total:400/400 with positive 303
   ACC:85%, Total:400/400 with positive 343

6. ACC:75%, Total:400/400 with positive 301
   ACC:80%, Total:400/400 with positive 320



The choice of the algorithm was weighted SVM since it is now of the strong classifier and was more suited for the data. Maximum-margin support vector machines (SVMs) generate a hyperplane which produces the clearest separation between positive and negative feature vectors. These SVMs are effective when datasets are large. However, when few training samples are available, the hyperplane is easily influenced by outliers that are geometrically located in the opposite class. Thus, I chose SVM which weights feature vectors to reflect the local density of support vectors and quantifies classification uncertainty in terms of the local classification capability of each training sample. 